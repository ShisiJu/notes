# 计算机中的字符编码(Unicode UTF-8 GBK)

## 计算机的字符都是在哪里定义的

答: 在字符集里定义的;

每个国家都有自己的语言, 每种语言又对应有很多不同的字符;
像英语只需要 100 多个字符就够了, 而中文则有非常多的字符(常用的 3000+个);

在计算机使用之初, 只需要支持英文就好;
所以早在 1963 年就有了 [ASCII 码](https://zh.wikipedia.org/wiki/ASCII);
它里面定义了英文字母,数字和一些字符;

[找一张 ascii 码的图片]

但随着计算机的流行, 中国也开始使用起来了计算机;
此时, 就必须要考虑中文字符了;

因此, 中国在 1980 年发布了[GB2312 (信息交换用汉字编码字符集·基本集)](https://zh.wikipedia.org/wiki/GB_2312); 里面收录了 6763 个比较常用的汉字;

我们知道世界上还有很多其它的文字, 在别的国家往往也会有一套类似中国 GB2312 的字符集;

如果有一个字符集能够包含世界上所有的字符, 人们都使用同一套字符集那该多好啊!
从而实现和谐的大统一.

[Unicode](https://zh.wikipedia.org/wiki/Unicode) 的出现也是出于这样的目的;
它整理、编码了世界上大部分的文字系统，使得电脑可以用更为简单的方式来呈现和处理文字。

## 计算机如何存储字符的

答: 根据不同的编码方式, 把字符存储成二进制的数据;

我们知道, 计算机只能存储二进制数据;
那么, 如何才能把字符存储成二进制数据呢?

我们在上一节知道了, 字符是在`字符集`中定义的;
字符集不仅仅有字, 而且还给每个字进行了编码; 每个字的编码称之为 `码点`;

我们以 Unicode 为例, 下面的内容取自[Unicode 与 JavaScript 详解](https://www.ruanyifeng.com/blog/2014/12/unicode.html)

Unicode 拥有非常多的字符, 并且 Unicode 还在继续发布新的版本, 加入新的字符;
因此, Unicode 是分区定义。

每个区可以存放 65536 个（2^16）字符，称为一个平面（plane）。
目前，一共有 17 个（2^5）平面， 也就是说，整个 Unicode 字符集的大小现在是 2^21。

最前面的 65536 个字符位，称为基本平面（缩写 BMP），它的码点范围是从 0 一直到 2^16-1，
写成 16 进制就是从 U+0000 到 U+FFFF。

所有最常见的字符都放在这个平面，这是 Unicode 最先定义和公布的一个平面。
剩下的字符都放在辅助平面（缩写 SMP），码点范围从 U+010000 一直到 U+10FFFF。

---

有了码点, 相当于给每个字加入了编号; 有了编号, 就可以通过二进制的数据表示了;

比如，中文 `严` 的码点是 十六进制的 `4E25`; 二进制可以用 `100111000100101` 来存储;

但是, 我们会发现转换成二进制数足足有 15 位，也就是说，这个符号的表示至少需要 2 个字节。
如果要表示其他更大的符号，可能需要 3 个字节或者 4 个字节，甚至更多。

并且, 如果要表示`英文字符a`, 它码点是`0061`, 如果直接使用 `1100001`表示,
那么, 11000`1100001`100111000100101 这种二进制数据, 如何拆分呢?

> 如果直接按照 4 个 16 进制数来存储的话, 常用的英文字符会有非常大的空间浪费;

所以, 我们要在计算机中存储字符, 有下面两个问题

1. 如何高效地利用存储空间, 用最少的数据, 表达最多的字符
2. 如何界定字符对应的二进制数据的边界

为了处理这两个问题, UTF-8（8-bit Unicode Transformation Format）出现了;
它是一种针对 Unicode 的可变长度字符编码;

UTF-8 的编码规则很简单，只有 2 条：

1. 对于单字节的符号：字节的第一位设为 0，后面 7 位为这个符号的 Unicode 码。因此对于英语字母，UTF-8 编码和 ASCII 码是相同的；
2. 对于 n 字节的符号（n > 1）：第一个字节的前 n 位都设为 1，第 n + 1 位设为 0，后面字节的前两位一律设为 10。剩下的没有提及的二进制位，全部为这个符号的 Unicode 码。

还是以 `严` 为例子吧; 它的 Unicode 编码是 `4e25`;
转化为二进制是`100111000100101`;

如果按照 uft-8 的编码格式, 它的二进制表达为
`11100100`- `10111000` - `10100101`

我们可以通过代码来间接看一下 (js 代码)

```js
// 严 的Unicode的十进制编码 20005
"严".codePointAt(0);
// 转换成16进制: 4e25
(20005).toString(16);
// 严
("\u4e25");
// 严的uft-8表达形式:  %E4%B8%A5
encodeURI("严");
// 转换成二进制看一下: 111001001011100010100101
(0xe4b8a5).toString(2);
```

## 编程中的字符集又有什么坑

编程语言中有默认的字符串编码方式; 这里以 ruby 为例, 它默认的编码方式是 UTF-8;

数据库也会指定编码方式; 但是, 要注意的是:
数据库有 3 种字符集设置

- 数据库本身, 也就是 DBMS serve 的字符编码
- 数据库 client, 通常来说是我们的程序对应的编码方式
- 会话字符集环境 (默认和 client 一致)

如果, 数据库和程序的编码一致, 那么一切安好, 天下太平;

这里举一个例子: 以 oracle 为例了;

假如, oracle server 设置的数据库编码为 `GBK`;
应用程序是 UTF-8, 同时 NLS_LANG(oracle 是 client 字符集编码对应的环境变量)是 UTF-8;

尽管数据库编码是 GBK, 而非 UTF-8,
大部分情况下, 你会发现系统中是没有字符乱码的;

这是为什么呢?

因为, 数据库会帮忙转码;
根据 client 的字符集是 UTF-8, 它会帮忙存储为 GBK;
所以, 大部分情况下是没有问题的;

但是, 注意这里是`大部分情况`;
为什么这么说呢?

因为, 一旦涉及到 GBK 没有, 但是 UTF-8 有的编码;
那么, 数据库就无法找到对应的字符; 从而, 会把字符替换成 `?` ;
导致了出现了数据库存储的乱码;

推荐阅读

[程序员必备：彻底弄懂常见的 7 种中文字符编码](https://zhuanlan.zhihu.com/p/46216008)

### 不同的编码方式是如何进行转码的

答: 通过 Unicode, 进行互相转换;

Unicode 与 GBK 字符集是不兼容的，同一个汉字在这两个字符集里的码值是完全不一样的。
那么, 怎么进行转码呢?

可以, 通过`Unicode` 这个中间字符集, 进行转码.
先让 GBK 中的字符, 转成 Unicode 的码点, 再通过 Unicode 转换成 UTF-8;

打一个比喻: GBK 是汉语, UTF-8 是西班牙语; 而 Unicode 是英语;
现在有汉英字典, 也有西班牙-英语字典;

如果我想要知道汉语对应的西班牙语, 那么汉语可以先翻译成英语, 再通过英语找到对应的西班牙语;

## 总结

- 字符集是字符的集合, 它给每个字符进行了编码, 也就是让每个字符有了独特的码点
- 字符编码是对字符集的转换, 字符编码让字符集可以在计算机中高效地表达
